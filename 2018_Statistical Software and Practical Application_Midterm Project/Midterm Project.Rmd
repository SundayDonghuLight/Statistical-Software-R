---
title: "Midterm Project"
author: 410411325 林皓翔，410511303 陳曼琳，410511331 李阜舫
output: 
  html_document: 
    toc: true
    toc_depth: 3
    toc_float: 
      collapsed: false
      smooth_scroll: false
---

###2. Referring to the webpage of Kernel density estimation on Wikipedia, do the following:
 <p>
####a.(10 pts) Describe what is called a kernel estimate of an unknown pdf. Also show that the kernel estimate $\widehat{f_{h}}$ of an unknown pdf $f$ is a legitimate pdf. That is, show that $\widehat{f_{h}}$ is nonnegative and its integral over R is one.
 <p>
>中文說明:<p>
核密度估計是一種無母數的估測方式用來估計一個隨機變數背後的機率密度函數。
從概念上來說，基於有限的數據樣本，我們使用一個kernel function去對每一筆觀測值生成以該點為中心的分配，然後將這些分配加總後取平均來作為所估測的母體分配。
事實上，在使用核密度估計時，還會用到一個平滑參數$h$，用來調控估測分配的平滑程度，$h$越小對每個觀測值就越敏感，越大則分配曲線就越平滑。當樣本的觀測值夠多時，我們可以試著使用較小的$h$來獲得更好的估測結果。<p>
English description:<p>
Kernel density estimation is a non-parametric way to estimate the probability density function of a random variable.
Conceptually, based a finite data sample, we use a kernel function generate a probability density distribution for each observe data, and then average the sum of these distributions, come as the population distribution estimation.
In fact, we also use a smoothing parameter $h$ to adjust the smoothness of the distribution when using kernel density estimation.
The smaller the $h$, the more the estimator will be sensitive to each observation, and when $h$ be larger, distribution curve will be more smoother.
We can try to use a smaller $h$ to get a better result when there are lots of data in the sample.

 <p>
>**To show that kernel estimate $\widehat{f_{h}}$ of an unknown pdf $f$ is a legitimate pdf:**<p>
Let $\left ( x_{1},x_{2},...,x_{n} \right )$ be a univariate independent and identically distributed sample drawn from some distribution with an unknown density $f$. We are interested in estimating the shape of this function $f$. Its kernel density estimator is
$$\widehat{f_{h}}\left ( x \right )= \frac{1}{n}\sum_{i=1}^{n}K_{h}\left ( x-x_{i}\right )= \frac{1}{nh}\sum_{i=1}^{n}K\left ( \frac{x-x_{i}}{h} \right )$$<p>
(i). $\widehat{f_{h}}\left ( x \right )\geq 0:$<p>
　　Since $K \geq 0$ and $h > 0$,　so $\frac{1}{nh}\sum_{i=1}^{n}K\left ( \frac{x-x_{i}}{h} \right ) \geq 0, \forall x$<p>
　　and hence $\widehat{f_{h}}\geq 0, \forall x$
 <p>
(ii). $\int_{-\infty}^{\infty}\widehat{f_{h}}\left ( x \right )dx =1 :$<p>
　　$\int_{-\infty}^{\infty}\widehat{f_{h}}\left ( x \right )dx =
\frac{1}{nh} \sum_{i=1}^{n}\int_{-\infty}^{\infty}K\left ( \frac{x-x_{i}}{h} \right )dx$<p>
　　　　　　　　　　　　　　　　　　　　(變數變換 $u= \frac{x-x_{i}}{h}$　$du= \frac{1}{h}dx$<p>
　　　　　　　 　　$=\frac{1}{n} \sum_{i=1}^{n}\int_{-\infty}^{\infty}K\left ( u \right )du$<p>
　　　　　　　 　　$=\frac{1}{n} \sum_{i=1}^{n}1$<p>
　　　　　　　 　　$=\frac{n}{n}=1$<p>
 
####b.(10 pts) Try to explain and distinguish the three terms: kernel, scaled kernel, and individual kernel.

>**Kernel:**<p>
Kernel is a non-negative function that integrates to one which we refer to construction of the estimated distribution. For most applications, it is desirable to define the function to satisfy two additional requirements:<p>
 - Normalization:<p>
　　$\int_{-\infty}^{\infty}K\left ( u \right )du=1$
 - Symmetry:<p>
　　$K\left ( -u \right )=K\left ( u \right )$ for all value of $u$<p>

>**Scaled kernel:**<p>
A kernel with subscript $h$ is called the scaled kernel and defined as $K_{h}\left ( x\right )= \frac{1}{h}K\left ( \frac{x}{h}\right )$, among $h$ > 0 is a smoothing parameter called the bandwidth.


>**Individual kernel:**<p>
Individual kernel means that we use the kernel function construct the distribution for each data from the sample individually. And the Individual kernels are these distribution, center respectively are the obersave data points.<p>


####c. (30 pts) Consider the data: −2.1,−1.3,−0.4,1.9,5.1,6.2 used in the illustrating example on Wikipedia, reproduce the density histogram and kernel density plot with normal kernels shown on that webpage. Compare it to the kernel density plot with the choice of default values of optional input arguments of R function density. Explain why they are not the same.

```{r, fig.width=9, fig.heigth=4}
rm(list=ls())
X <- c(-2.1, -1.3, -0.4, 1.9, 5.1, 6.2)
par(mfrow=c(1,2))

hist(X,xlim=c(-6,10),ylim=c(0,0.18),freq=F,main="",
xlab="x",ylab="Density function",border="slateblue2")
box(bty="o")
rug(X)

f <- density(X,1.5)
plot(f,xlim=c(-6,10),ylim=c(0,0.18),col='blue',lwd=3,main="",
xlab="x",ylab="Density function")
rug(X)
fx <- function(x,i){
	dnorm(x,X[i],1.5)/6
}
for(i in c(1:6)){
	curve(fx(x,i),add=T,col='red',lty=2,lwd=2)
}

plot(f,xlim=c(-6,10),ylim=c(0,0.18),col='blue',lwd=3,main="Wikipedia")
rug(X)

plot(density(X),xlim=c(-6,10),ylim=c(0,0.18),lwd=3,main="Default")
rug(X)

```

>Two graphs are use the same kernel, normal, but different bandwidth h.
The figure which use default values of optional input arguments of R function density has a larger h than the wikipedia one, hence its density curve is more smoothly. And the wikipedia one is more sensitive to each observation, it’s a obviously bimodal distribution.<p>
　　<p>
　　<p>

###4. In the law firm Tybo and Associates, there are six partners. Listed below is the number of cases each associate actually tried in court last month.

####4a.(5 pts) How many different samples of 3 associates are possible?
```{r }
choose(6,3)
```
####4b.(20 pts) List all possible samples of three associates, and the corresponding number of cases, and compute/list the sample sum and sample mean of the number of cases in each sample.
```{r}
A=c("Ruud","Wu","Sass","Flores","Wilhelms","Schueller")
B=c(3,6,3,3,0,1)
Z=NULL
i=1
n=1
while(i<=4){
  j=i+1
  while(j<=5){
    k=j+1
    while(k<=6){
      cat(c(A[i],A[j],A[k]),"\n") 
      cat(c(B[i],B[j],B[k]),"\n")
      cat("sum=",sum(c(B[i],B[j],B[k])),"\n")
      Z[n]=mean(c(B[i],B[j],B[k]))
      cat("mean=",Z[n],"\n")
      n=n+1
    k=k+1}
  j=j+1}
i=i+1}
```
####4c.(5 pts) Compare the mean of the distribution of sample means to the population mean.
```{r}
cat("population mean(3,6,3,3,0,1)=",mean(c(3,6,3,3,0,1)),"\n")

plot(x=c(1:20),
     y=Z,
     main="All values of sample means",
     xlab="different samples of 3 associates",
     ylab = "mean")
abline(h=mean(c(3,6,3,3,0,1)),
       col="red")
legend(14.1,4.07,
       c("sample mean.","population mean."),
       pch = c(1,NA),
       lty = 0:1,
       lwd = 1.0,
       col = c("black","red"))

plot(x=c(1:20),
     y=Z-mean(c(3,6,3,3,0,1)),
     main="Residuals of sample mean",
     xlab="different samples of 3 associates",
     ylab = "residuals of sample mean")
abline(h=0,col="red")
legend(14.1,1.4,
       c("sample mean.","population mean."),
       pch = c(1,NA),
       lty = 0:1,
       lwd = 1.0,
       col = c("black","red"))

cat(y=Z-mean(c(3,6,3,3,0,1)))
```
>由殘差圖可以看出,比population mean大和比population mean小的sample mean個數是一樣多的,且由殘差圖中可觀察出,sample mean在同一個刻度上,刻度為正的個數和刻度為負的個數是相同的。

####4d.(35 pts) Compare the dispersion in the population with that of the sample means based on a chart similar to the side-by-side probability histograms for discrete random variables given below,
```{r, fig.width=9, fig.heigth=4}
par(mfrow=c(1,2))

Population_Distribution = c(3,6,3,3,0,1)
means_freq = table(Population_Distribution)
mf = data.frame(means_freq)
plot(as.vector(mf[[1]]),mf[[2]]/6,
     type='h',
     xlim = c(0.0,6.5),
     ylim=c(0.0,0.6),
     main = "Population Distribution",
     xlab = "Number of Cases",
     ylab = "Probability",
     col="purple2",
     lwd=5,
     las=1)
summary(Population_Distribution)
points(2.667,-0.018,col="red",pch=19)
text(x=2.665, y=0.05,expression(mu),col = "red")

Distribution_of_Sample_Mean= c(4,4,3,10/3,3,2,7/3,2,7/3,4/3,4,3,10/3,3,10/3,7/3,2,7/3,4/3,4/3)
means_freq = table(Distribution_of_Sample_Mean)
mf = data.frame(means_freq)
plot(as.vector(mf[[1]]),mf[[2]]/20,
     type='h',
     xlim = c(1.0,4.5),
     ylim=c(0.0,0.25),
     main = "Distribution of Sample Mean",
     xlab = "Sample mean of 3 associates",
     ylab = "Probability",
     col="purple2",
     lwd=5,
     las=1)
summary(Distribution_of_Sample_Mean)
points(2.667,-0.007,col="red",pch=19)
text(x=2.665, y=0.02,expression(mu*bar(x)),col = "red")


```

>由此圖可以看出population mean和mean of sample mean是一樣的。比較右圖和左圖,可以發現到sample mean的分佈比population的分布還集中。

####4e.(5 pts) Using the concept that the mean of a random variable is the average of its possible values weighted by the probability mass, compute and compare the population mean and the mean of the sample mean.
```{r}
cat("population mean=",mean(Population_Distribution),"\n")
cat("mean of the sample mean=",mean(Distribution_of_Sample_Mean))
```
>以mean of sample means來看,$\frac{\sum_{i=1}^{20}(ith.sample.mean)}{20}= \frac{\sum_{i=1}^{20}(partial.sum.of.sample.i)}{60}= \frac{10*(sum.of.everyone.cases)}{60}=\frac{1}{6}*(sum.of.everyone.cases)$<p>
所以,mean of sample means會等於population mean。


####4f.(10 pts) Compare the variance of the distribution of sample means to the population variance.
```{r}
cat("population variance(3,6,3,3,0,1)=",var(c(3,6,3,3,0,1)*5/6),"\n")
cat("variance of sample means=",var(Z)*19/20,"\n")
```
>因為population mean= 2.666667 =mean of the sample mean,<p>
但是population variance=2.962963 > 0.7111111 = variance of sample means,
因此可以由variance的性質可知,population 的分布相較於sample means的分布還要分散。<p>
　　<p>
　　<p>

###5. Refer to the Real Estate data, which report information on the homes sold in the Goodyear, Arizona, area last year. Assume this to be the population. Do not edit/modify the content of the data file using softare other than R. Read the data file into a data frame using R function  read.table with appropriate specification of input arguments. Answer the following with help of R.
<p>
First, I read the data file into a data frame. 
```{r}
NVRE <- read.table("North_Valley_Real_Estate.csv", header=TRUE, sep=",")
```
<p>
####a.
####(i) How many observations are there in the data file?
>There are 105 observations in this data file.
<p>

####(ii) Take a look at the structure of the data frame. How many variables have missing values?
```{r}
str(NVRE)
```
>10 variables have missing values. (They are X, X.1,..., X.9, respectively.)
<p>

####(iii) Determine if all the observed values of these variables are missing.
```{r}
sum(is.na(NVRE$X))
sum(is.na(NVRE$X.1))
sum(is.na(NVRE$X.2))
sum(is.na(NVRE$X.3))
sum(is.na(NVRE$X.4))
sum(is.na(NVRE$X.5))
sum(is.na(NVRE$X.6))
sum(is.na(NVRE$X.7))
sum(is.na(NVRE$X.8))
sum(is.na(NVRE$X.9))
```
>Yes. Because there are not any observed values recorded in all of the 10 variables.
<p>

####(iv) Remove these variables using R functions rm and within.
```{r}   
within(NVRE, rm(X, X.1, X.2, X.3, X.4, X.5, X.6, X.7, X.8, X.9))
```
<p>

####b. Compute the mean and the standard deviation of the selling prices.
```{r}
mean(NVRE$Price)
sd(NVRE$Price)
```
>The mean of the selling prices is 357026.5; the standard deviation of the selling prices is 160700.1.
<p>

####c. Select randomly a sample of 10 homes using certain R function. Compute the mean of the sample.
```{r}
S <- sample(NVRE$record, 10)
cat("sample:",S,"\n")

i <- 1
sum <- 0
for(i in 1:10)
{
  sum <- sum+NVRE[S[i],"Price"]
}

cat("sample mean:", sum/10)
```
<p>

####d. Determine the likelihood of a sample mean this large or larger from the population based on an approximate method that takes into account that the observed sample is selected from a finite population without replacement.
>I randomly draw 100000 samples of sample size 10 from the population. Also, I calculate their respective sample means. Then I set a counter to count the number of sample means which are larger than population mean. Finally I calculate its proportion.
```{r}
i <- 1
j <- 1
k <- 0 
m <- 100000 #sampling frequency
n <- 10 #sample size 
for(j in 1:m)
{
  S <- sample(NVRE$record, n, replace=FALSE)
  sum <- 0
  
  for(i in 1:n)
  {
    sum <- sum+NVRE[S[i],"Price"]
  }
  mean.s <- sum/n
  
  if(mean.s>357026.5)
  {
    k <- k+1
  }
}
cat("the number of sample means which are larger than population mean:",k,"(times)\n")
cat("proportion of sample means which are larger than population mean:", k/m)
```
<p>

####e. 
####(i) Justify the appropriateness of the approximate method you adopted in computing the above likelihood by conducting a study on the sampling distribution of the sample means for small samples taken from the given finite population.
>我想用Central Limit Theorem來解釋5.(d)：<p>
><p>
>使用這個定理有一個很重要的前提是，每組sample裡的sample point必須是independent。但sampling without replacement這個方法會因為某個sample point被抽取而影響下一個sample point被抽取的機率，所以sample point是dependent。雖說如此，我們還是能藉由控制sample size，讓其小於10%的population，來有效排除dependent sample point。
><p>
>（考量到整個population有105筆紀錄，於是在處理5.(d)時，我選擇10作為sample size。）
<p>

####(ii) Summarize your findings.
>(設m: sampling frequency; n:sample size; p: probability that a sample mean is larger than population mean)
><p>
>**＊當m越大，估算出來的p浮動越小。**
<p>
固定n=10，分別重複5次用 m1=10, m2=100, m3=1000, m4=10000, m5=100000 去算p，以下是程式執行的結果：
><p>
><p>
>(m1=10)     p1=0.6,     p2=0.6,     p3=0.6,     p4=0.2,     p5=0.7 <p>
(m2=100)    p1=0.55,    p2=0.42,    p3=0.45,    p4=0.45,    p5=0.33 <p>
(m3=1000)   p1=0.444,   p2=0.483,   p3=0.481,   p4=0.451,   p5=0.477 <p>
(m4=10000)  p1=0.4692,  p2=0.4736,  p3=0.4772,  p4=0.4773,  p5=0.4801 <p>
(m5=100000) p1=0.47184, p2=0.47498, p3=0.47237, p4=0.47213, p5=0.4725 <p>
<p>
<p>
<p>
<p>
<p>